---
title: "About"
author: ["Carl Henrik Ek"]
lastmod: 2023-11-10T20:49:51+00:00
draft: false
weight: 3002
title: "About"
---
### Vision and scope

The CDT will enable students to develop new fundamental AI capabilities in the context of a diversity of complex systems. Rather than working in isolation, as is usual in AI, the students will learn to develop these in a collaborative manner tied to a specific application domain. In particular, the CDT will comprise three topics:

Uncertainty in complex systems (UQ). Human and AI collaborative decision making requires principled uncertainty quantification. The CDT will develop leading-edge probabilistic machine learning methods, leveraging uncertainty in a statistical manner to drive the exploration of new parameter spaces and promote scientific discovery. With a focus on the methodological and theoretical aspects, the CDT will provide impact across any field where decision-making is critical. Uncertainty quantification and modelling will underpin the following two topics of the CDT:

Decision-making with humans in the loop (DMHL).  By acknowledging that the human user, in many cases, is unable to fully specify the details computer systems require, and by jointly modelling the machine learning task and the user, AI technologies will be more efficient in addressing key challenges such as experimental design from scarce data and domain shift, as well as promoting trust in AI-enabled systems.

Decision-making for ML systems (DMML). Increasingly, several automated decision-making systems are being built by a composition of individual machine learning components making decisions at the component level. For scientific systems that produce huge data volumes, so-called “big science” (e.g., SKA, CERN), AI-driven decisions are increasingly necessary to replace human decisions at multiple points within scientific analyses and facility operations. The CDT will look at automated AI approaches that can ensure such systems combination is robust, safe and accurate.

Model interpretability and explainability will be transversal to the three topics above. Decision making with AI needs to be interpretable and explainable to facilitate interrogation of decision processes such that trust can be built by the human, and it is essential for understanding and meeting ethical and legal implications.
